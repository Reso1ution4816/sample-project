{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有一个线性回归模型 $f(\\mathbf{X}; \\mathbf{w}) = \\mathbf{X}\\mathbf{w}$ <br>\n",
    "\n",
    "其中$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}; \\mathbf{y} \\in \\mathbb{R}^{m}$; $\\mathbf{w}$是模型参数. <br>\n",
    "\n",
    "注意$\\mathbf{X} = (\\underbrace{[1,...,1]^T}_{m} | \\mathbf{X_{samples}} )$. 即我们的样本$\\mathbf{X_{samples}} \\in \\mathbb{R}^{m \\times (n-1)} $<br>\n",
    "\n",
    "在训练过程中我们想要最小化这个损失函数:<br>\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2\n",
    "\\end{equation} <br>\n",
    "\n",
    "在求 $argmin_{\\mathbf(w)}J(\\mathbf{w})$ 时我们可以利用其显式解: <br>\n",
    "\n",
    "\\begin{equation}\n",
    "(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} = argmin_{\\mathbf(w)}||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2\n",
    "\\end{equation} <br>\n",
    "\n",
    "但在 $\\mathbf{X}^T\\mathbf{X}$ 是不可逆矩阵时该方法不适应, 即$\\mathbf{X}^T\\mathbf{X}$是奇异矩阵时 (iff $det \\mathbf{X}^T\\mathbf{X} = 0$). 我们可以考虑用梯度下降方法求该问题的数值解 (numerical solution).<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降方法基于以下的观察:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果实值函数 $F({\\mathbf{x}})$在点 $\\mathbf{u}$ 处可微且有定义，那么函数 $F({\\mathbf{x}})$在 $\\mathbf{u}$ 点沿着梯度相反的方向 $ -\\nabla F({\\mathbf{u}})$ 下降最快.<br>\n",
    "\n",
    "\n",
    "因而，如果 $ {\\mathbf{u_{new}}}={\\mathbf{u}}-\\alpha \\nabla F({\\mathbf{u}})$ 对一个足够小的数值 $ \\alpha >0$ 成立，那么 $F({\\mathbf{u_{new}}}) \\leq F({\\mathbf{u}})$ .<br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令 $F(\\mathbf{v}) = || \\mathbf{v} ||^2_2$, $\\mathbf{v} \\in \\mathbb{R}^m$, 由于 $|| \\cdot ||^2_2$ 处处可微: <br>\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla F(\\mathbf{v}) & = \\frac{\\partial }{\\partial \\mathbf{v}} F(\\mathbf{v}) = \\Bigg[ \\frac{\\partial }{\\partial \\mathbf{v_1}} F(\\mathbf{v}),  \\frac{\\partial }{\\partial \\mathbf{v_2}} F(\\mathbf{v}), ..., \\frac{\\partial }{\\partial \\mathbf{v_n}} F(\\mathbf{v})\\Bigg]^T\n",
    "\\end{align}<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial \\mathbf{v_i}} F(\\mathbf{v}) & = \\frac{\\partial }{\\partial \\mathbf{v_i}} ||\\mathbf{v}||^2_2 \\\\\n",
    "& = \\frac{\\partial }{\\partial \\mathbf{v_i}} \\sum_{k=1}^{m} \\mathbf{v_k^2} \\\\\n",
    "& = 0+0+...+2\\mathbf{v_i}+0 \\\\\n",
    "& = 2\\mathbf{v_i}\\\\\n",
    "\\end{align} <br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla F(\\mathbf{v}) = \\frac{\\partial }{\\partial \\mathbf{v}} F(\\mathbf{v}) = 2\\mathbf{v}\n",
    "\\end{equation}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回到回归问题:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的损失函数 $J(\\cdot)$ 只和模型参数 $\\mathbf{w}$ 有关, 因为在计算损失时 $\\mathbf{X}$ 和 $\\mathbf{y}$是已知的, 我们需要通过调整$\\mathbf{w}$ 来优化模型<br>\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2\n",
    "\\end{equation} <br>\n",
    "\n",
    "我们可以考虑使用梯度下降法, $\\alpha$ 是一个参数 <br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w_{new}} = \\mathbf{w_{old}} -\\alpha \\nabla J({\\mathbf{w_{old}}})\n",
    "\\end{equation} <br>\n",
    "\n",
    "其中 <br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J(\\mathbf{w}) = \\Bigg[ \\frac{\\partial }{\\partial \\mathbf{w_1}} J(\\mathbf{w}),  \\frac{\\partial }{\\partial \\mathbf{w_2}} J(\\mathbf{w}), ..., \\frac{\\partial }{\\partial \\mathbf{w_n}} J(\\mathbf{w}) \\Bigg] ^ T\n",
    "\\end{equation}<br><br>\n",
    "\n",
    "\n",
    "对于$\\mathbf{w}$的每一个元素 <br>\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial \\mathbf{w_i}} J(\\mathbf{w}) & = \\frac{\\partial }{\\partial \\mathbf{w_i}} ||\\mathbf{X}\\mathbf{w} - \\mathbf{y}||^2_2  \\\\\n",
    "& = \\frac{\\partial }{\\partial \\mathbf{w_i}}  F(G(\\mathbf{w}))\\\\\n",
    "& =  \\frac{\\partial }{\\partial G} F(G(\\mathbf{w})) \\cdot  \\frac{\\partial }{\\partial \\mathbf{w_i}}  G(\\mathbf{w}) \\\\\n",
    "& \\\\\n",
    "& = 2 G(\\mathbf{w}) \\cdot  \\mathbf{X_{:,i}} \\\\\n",
    "& \\\\\n",
    "& = 2 (\\mathbf{X}\\mathbf{w} - \\mathbf{y}) \\cdot \\mathbf{X_{:,i}} \\\\\n",
    "& \\\\\n",
    "& \\\\\n",
    " F(\\mathbf{v}) &= ||\\mathbf{v}||^2_2  \\quad \\frac{\\partial }{\\partial \\mathbf{v}} F(\\mathbf{v}) = 2\\mathbf{v} \\\\\n",
    " G(\\mathbf{w}) & = \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\quad  \\frac{\\partial }{\\partial \\mathbf{w_i}} G(\\mathbf{w}) = \\mathbf{X_{:,i}}\n",
    "& \\\\\n",
    "& \\\\\n",
    "\\end{align} \n",
    "<br><br>\n",
    "\n",
    "所以 <br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J(\\mathbf{w}) = 2 (\\mathbf{X}\\mathbf{w} - \\mathbf{y}) \\cdot  \\mathbf{X}\n",
    "\\end{equation}<br><br>\n",
    "\n",
    "\n",
    "那么模型中$\\mathbf{w}$可以由梯度下降法搜索得到: ($\\alpha$ 是一个参数) <br><br>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{w_{new}} & = \\mathbf{w_{old}} - 2 \\alpha (\\mathbf{X}\\mathbf{w_{old}} - \\mathbf{y}) \\cdot \\mathbf{X} \\\\\n",
    "&\\\\\n",
    "& = \\mathbf{w_{old}} - 2 \\alpha (\\mathbf{y_{(\\text{predicted on }X)}} - \\mathbf{y}) \\cdot \\mathbf{X}\n",
    "\\end{align} <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用迭代的方式来计算梯度并更新 $\\mathbf{w_{new}}$ 即:<br>\n",
    "`\n",
    "n_iters = 1000\n",
    "w = [1,1,1]\n",
    "for i in range(n_iters):\n",
    "    w = w - 2 * alpha * gradient_of_J(w)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成下面的 `gradient_descent`函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class MyLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    @staticmethod\n",
    "    def ones_augment_to_left(X):\n",
    "        X = np.array(X)\n",
    "        ones = np.ones(X.shape[0])\n",
    "        return np.column_stack([ones, X])\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_descent(X, y, n_iters=10000, alpha=0.05, weight=None):\n",
    "        w = weight\n",
    "        if w is None:\n",
    "            w = np.random.rand(X.shape[1])\n",
    "            w = np.ones(X.shape[1])\n",
    "        pass\n",
    "        \n",
    "        ###### write your code below ######\n",
    "        for i in range(1, n_iters+1):\n",
    "            y_pred = X.dot(w)\n",
    "            loss = y_pred - y\n",
    "            \n",
    "            grad = loss.dot(X)/X.shape[0]\n",
    "            w = w - alpha *  grad # update\n",
    "                \n",
    "        ###### write your code above ######\n",
    "        \n",
    "        return w\n",
    "    \n",
    "    @staticmethod\n",
    "    def closed_form(X ,y):\n",
    "        product = np.dot(X.T, X)\n",
    "        theInverse = np.linalg.inv(product)\n",
    "        return np.dot(np.dot(theInverse, X.T), y)\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, method='closed form', **kwargs):\n",
    "        X = self.ones_augment_to_left(X_train)\n",
    "        y = np.array(y_train)\n",
    "        \n",
    "        if method=='closed form':\n",
    "            self.w = self.closed_form(X ,y)\n",
    "        elif method == 'gradient descent':\n",
    "            self.w = self.gradient_descent(X, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        augX_test = self.ones_augment_to_left(X_test)\n",
    "        return augX_test.dot(self.w)\n",
    "    \n",
    "# 测试\n",
    "import numpy as np\n",
    "\n",
    "mlr = MyLinearRegression()\n",
    "\n",
    "X = np.array([[1, 5], [3, 2], [6, 1]])\n",
    "y = np.array([2, 3, 4])\n",
    "y_pred = mlr.fit(X, y, method='gradient descent', \n",
    "                 n_iters=10000, \n",
    "                 alpha=0.05).predict(X)\n",
    "print('fitted w is \\t', mlr.w)\n",
    "print('expected w is \\t [ 2.42857143  0.28571429 -0.14285714]')\n",
    "print('Am I correct? \\t', np.isclose(y, y_pred, atol=1e-2).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
